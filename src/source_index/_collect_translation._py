import os, errno, sys, re
import json
import collections
import argparse
from clang import cindex

parser = argparse.ArgumentParser(description="Collects translations form source files")

parser.add_argument(
    "--target",
    "-t",
    dest="target",
    type=str,
    required=True,
    help="Name of target unit to be created",
)

parser.add_argument(
    "--inputs",
    "-i",
    dest="input_files",
    type=str,
    nargs="+",
    required=True,
    help="List of input files to be parsed",
)

parser.add_argument(
    "--input-dir",
    "-d",
    dest="input_dir",
    type=str,
    required=True,
    help="List of input dirs where files can be located",
)

parser.add_argument(
    "--include-dirs",
    "-I",
    dest="include_dirs",
    type=str,
    nargs="+",
    required=False,
    help="List of include dirs",
)

parser.add_argument(
    "--output-dir",
    "-o",
    dest="out_dir",
    type=str,
    required=True,
    help="Directory where translations will be collected and updated from",
)

parser.add_argument(
    "--clang-dir",
    "-c",
    dest="clang_dir",
    type=str,
    required=False,
    help="Directory where translations will be collected and updated from",
)

TRANSLATION_TOKEN_LIST = ["_TR"]

MacroNode = collections.namedtuple(
    "MacroNode", ["spelling", "tokens", "offset", "cursor"]
)


class TranslationLiteralRecord:
    def __init__(self, source, translation=None, disambiguation=None):
        self.source = source
        self.translation = translation if translation else source
        self.disambiguation = (
            disambiguation if disambiguation else None
        )  # opt-out empty string
        self._order = 0

    def __eq__(self, other):
        assert isinstance(other, TranslationLiteralRecord)
        return self.source == other.source and self.source == other.source

    def __ne__(self, other):
        assert isinstance(other, TranslationLiteralRecord)
        return not self.__eq__(other)

    def __hash__(self):
        return hash(self.source) ^ hash(self.disambiguation)

    def __repr__(self):
        return "TranslationLiteralRecord({})".format(vars(self).__repr__())

    # TODO: Better serialization / deserialization from and to json
    def to_json(self):
        return dict(filter(lambda x: not x[0].startswith("_"), vars(self).items()))

    # TODO: Better serialization / deserialization from and to json
    @classmethod
    def from_json(cls, _json):
        return cls(**_json)

    @classmethod
    def from_tokens(cls, tokens):
        if len(tokens) not in [4, 6]:
            # TODO: Print line and col number where it happened
            raise RuntimeError(
                'The "tokens" array may be either 4 or 6 items long. Length:{} Content:{} '.format(
                    len(tokens), " ".join(tokens)
                )
            )
        if tokens[1] != "(" and tokens[-1] != ")":
            # TODO: Print line and col number where it happened
            raise RuntimeError("Syntax error. {}".format(" ".join(tokens)))

        if tokens[2][0] != '"' and tokens[2][-1] != '"':
            # TODO: Print line and col number where it happened
            raise RuntimeError("Syntax error. {}".format(" ".join(tokens)))

        if len(tokens) == 6 and tokens[4][0] != '"' and tokens[4][-1] != '"':
            # TODO: Print line and col number where it happened
            raise RuntimeError("Syntax error. {}".format(" ".join(tokens)))

        return cls(
            source=tokens[2][1:-1],
            disambiguation=tokens[4][1:-1] if len(tokens) == 6 else None,
        )


def index_collect_macros(node):
    assert isinstance(node, cindex.Cursor)

    if node.kind is cindex.CursorKind.MACRO_INSTANTIATION:
        yield MacroNode(
            spelling=node.spelling,
            tokens=[token.spelling for token in node.get_tokens()],
            offset=node.extent.start.offset,
            cursor=node,
        )

    for child in node.get_children():
        yield from index_collect_macros(child)


def find_matching_tokens(node, token_list):
    assert isinstance(node, cindex.Cursor)
    assert token_list
    return filter(lambda x: x.spelling in token_list, index_collect_macros(node))


def collect_translation_literals(nodes):
    records = set()
    count = 0
    for node in nodes:
        assert isinstance(node, MacroNode)
        record = TranslationLiteralRecord.from_tokens(node.tokens)
        record._order = 2 * count + 1
        if record not in records:
            records.add(record)
            yield record
            count += 1


def ordered_diff_literals(left, right):
    _left, _right = frozenset(left), frozenset(right)
    return sorted(
        [item for item in (_left & _right) | (_left - _right)], key=lambda o: o._order
    )


def _make_dirs(path):
    try:
        os.makedirs(path)
    except OSError as e:
        if e.errno == errno.EEXIST and os.path.isdir(path):
            pass
        else:
            raise


def _parse_language_literals(_json):
    count = 0
    for literal_json in _json:
        literal = TranslationLiteralRecord.from_json(literal_json)
        literal._order = 2 * count
        yield literal
        count += 1


def _parse_language_contexts(_json):
    result = {}
    for context_json in _json:
        assert "context" in context_json
        assert "translations" in context_json
        # TODO: Add warn here that file may be corrupted or add error recovery options
        yield (
            context_json["context"],
            _parse_language_literals(context_json["translations"]),
        )

    return result


def _load_language_files(language_dir):
    if not os.path.isdir(language_dir):
        return {}

    result = {}
    for language_file in os.listdir(language_dir):
        matches = re.match(
            r"(.*)\.(?P<language_id>\w{2}(?:-\w{2})?)\.json", language_file
        )
        result[matches.group("language_id")] = set()
        if matches:
            try:
                with open(os.path.join(args.out_dir, language_file), "r") as f:
                    language_json = json.load(f)
            except:
                # TODO: Add warn here that file may be corrupted
                continue

            result[matches.group("language_id")] = dict(
                _parse_language_contexts(language_json)
            )

    return result


def _parse_source_file(source_file_path):
    index = cindex.Index.create()
    compiler_args = ["-std=c++17"]
    if args.include_dirs:
        compiler_args.extend(
            "-I{}".format(include_dir) for include_dir in args.include_dirs
        )

    return index.parse(
        source_file_path,
        args=compiler_args,
        options=cindex.TranslationUnit.PARSE_DETAILED_PROCESSING_RECORD,
    )


class MyJsonEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, TranslationLiteralRecord):
            return obj.to_json()
        return json.JSONEncoder.default(self, obj)


def main():
    args = parser.parse_args()

    if args.clang_dir:
        cindex.Config.set_library_path(args.clang_dir)

    # -- Gather list of files
    input_files = []
    if not os.path.isdir(args.input_dir):
        print("Input directory not exists: {}".format(args.input_dir))
        sys.exit(-1)

    for input_file in args.input_files:
        file_path = os.path.join(args.input_dir, input_file)
        if os.path.isfile(file_path):
            input_files.append(file_path)

    # -- load previous translations (if any)
    stored_language_packages = _load_language_files(args.out_dir)

    if not stored_language_packages:
        stored_language_packages = {"en-us": {}}

    # -- Scan all files
    collected_contexts = {}
    for file_path in input_files:
        translation_unit = _parse_source_file(file_path)

        context_name = os.path.basename(file_path)

        print("Parsing {}".format(context_name))

        collected_nodes = find_matching_tokens(
            translation_unit.cursor, TRANSLATION_TOKEN_LIST
        )
        # We have to evaluate it here anyways
        collected_contexts[context_name] = frozenset(
            collect_translation_literals(collected_nodes)
        )

    # -- diff
    updated_translations = {}
    for language_id, contexts in stored_language_packages.items():
        updated_contexts = {}
        for context_name in collected_contexts.keys():
            if context_name not in contexts:
                updated_contexts[context_name] = ordered_diff_literals(
                    collected_contexts[context_name], []
                )
            else:
                updated_contexts[context_name] = ordered_diff_literals(
                    collected_contexts[context_name], contexts[context_name]
                )

            updated_translations[language_id] = updated_contexts

    # -- Write
    # TODO: Loop though all locale ids

    _make_dirs(args.out_dir)
    for language_id, contexts in updated_translations.items():
        out_filename = "{}.{}.json".format(args.target, language_id)

        print("Writing {} ".format(out_filename))

        translations = [
            {"context": context_name, "translations": translation_set}
            for context_name, translation_set in contexts.items()
        ]

        out_file = os.path.join(args.out_dir, out_filename)
        with open(out_file, "w") as file:
            json.dump(translations, file, indent=2, sort_keys=False, cls=MyJsonEncoder)
            file.write("\n")
